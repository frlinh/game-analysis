{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a5f3c49",
   "metadata": {},
   "source": [
    "### Q: is the champion easy or difficult to master? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "507f7187",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26460/3537037493.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msvm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSVC\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# Import our input dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# Import our dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import our input dataset\n",
    "mastery_df = pd.read_csv('championMasteryBeta.csv')\n",
    "mastery_df.head()\n",
    "\n",
    "# Logistic Regression\n",
    "Classification algorithm that can analyze continuous and categorical variables. Predicts the probability of the input data belonging to one of two groups\n",
    "* can be overwhelmed by too much data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dc7ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mastery_df = mastery_df.drop(columns='Date')\n",
    "mastery_df = mastery_df.drop(columns='tokensEarned')\n",
    "mastery_df = mastery_df.drop(columns='chestGranted')\n",
    "mastery_df = mastery_df.drop(columns='summonerId') #most likely keep this later on to merge our diamondPlayer dataset with this one \n",
    "mastery_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7946015d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8908\\2122376198.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#split training/test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "#define x and y\n",
    "X = mastery_df.drop(columns='championPointsUntilNextLevel')\n",
    "y= mastery_df.championPointsUntilNextLevel\n",
    "\n",
    "#split training/test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bad8d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393a722f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the logistic regression model\n",
    "log_classifier = LogisticRegression(solver=\"lbfgs\",max_iter=200)\n",
    "\n",
    "# Train the model\n",
    "log_classifier.fit(X_train,y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = log_classifier.predict(X_test)\n",
    "print(f\" Logistic regression model accuracy: {accuracy_score(y_test,y_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542ba070",
   "metadata": {},
   "source": [
    "# vs. Basic Neural Network\n",
    "* prone to overfitting and can be more difficult to train than a logistic regression model \n",
    "* thrives with a lot of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a6525a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the basic neural network model\n",
    "nn_model = tf.keras.models.Sequential()\n",
    "nn_model.add(tf.keras.layers.Dense(units=16, activation=\"relu\", input_dim=8))\n",
    "nn_model.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the Sequential model together and customize metrics\n",
    "nn_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "fit_model = nn_model.fit(X_train_scaled, y_train, epochs=50)\n",
    "\n",
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn_model.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b29245",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM)\n",
    "binary classifier that distinguishes data points from 2 separate groups. tries to maximize the distance between the 2 closest data points of the groups.\n",
    "* can build adequate models with linear/nonlinear data unlike logistic regression\n",
    "* can analyze and interpret multiple data types\n",
    "* less prone to overfitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f6ebac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dataset\n",
    "mastery_df = pd.read_csv('championMasteryBeta.csv')\n",
    "mastery_df.head()\n",
    "\n",
    "#drop the columns we don't need\n",
    "mastery_df = mastery_df.drop(columns='Date')\n",
    "mastery_df = mastery_df.drop(columns='tokensEarned')\n",
    "mastery_df = mastery_df.drop(columns='chestGranted')\n",
    "mastery_df = mastery_df.drop(columns='summonerId') #most likely keep this later on to merge our diamondPlayer dataset with this one \n",
    "mastery_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6daaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the # of unique items in each column\n",
    "mastery_df.nunique()\n",
    "\n",
    "#bucket championLevel\n",
    "\n",
    "#bucket championPoints\n",
    "\n",
    "#bucket lastPlayTime\n",
    "\n",
    "#bucket championPointsSinceLastLevel\n",
    "\n",
    "#bucket championPointsUntilNextLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde1c506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a OneHotEncoder instance\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Fit and transform the OneHotEncoder \n",
    "encode_df = pd.DataFrame(enc.fit_transform(mastery_df))\n",
    "\n",
    "# Add the encoded variable names to the dataframe\n",
    "encode_df.columns = enc.get_feature_names(mastery_df)\n",
    "encode_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9be411c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge the new encode_df with the diamondPlayerIDs dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690dc085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define x and y\n",
    "y = \n",
    "X = \n",
    "\n",
    "# Split training/test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, stratify=y)\n",
    "\n",
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f715f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the SVM model\n",
    "svm = SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51894131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0826723a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "y_pred = svm.predict(X_test_scaled)\n",
    "print(f\" SVM model accuracy: {accuracy_score(y_test,y_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b2d5d3",
   "metadata": {},
   "source": [
    "# vs Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6778fb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model - deep neural net\n",
    "number_input_features = len(X_train_scaled[0])\n",
    "hidden_nodes_layer1 = 10\n",
    "hidden_nodes_layer2 = 5\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(\n",
    "    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\")\n",
    ")\n",
    "\n",
    "# Second hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "\n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the Sequential model together and customize metrics\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f275290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model \n",
    "fit_model = nn.fit(X_train_scaled, y_train, epochs=50) \n",
    "# Evaluate the model using the test data \n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1466ce53",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dc3c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dataset\n",
    "mastery_df = pd.read_csv('championMasteryBeta.csv')\n",
    "mastery_df.head()\n",
    "\n",
    "#drop the columns we don't need\n",
    "mastery_df = mastery_df.drop(columns='Date')\n",
    "mastery_df = mastery_df.drop(columns='tokensEarned')\n",
    "mastery_df = mastery_df.drop(columns='chestGranted')\n",
    "mastery_df = mastery_df.drop(columns='summonerId') #most likely keep this later on to merge our diamondPlayer dataset with this one \n",
    "mastery_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effedb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the # of unique items in each column\n",
    "mastery_df.nunique()\n",
    "\n",
    "#bucket championLevel\n",
    "\n",
    "#bucket championPoints\n",
    "\n",
    "#bucket lastPlayTime\n",
    "\n",
    "#bucket championPointsSinceLastLevel\n",
    "\n",
    "#bucket championPointsUntilNextLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0a2037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a OneHotEncoder instance\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Fit and transform the OneHotEncoder \n",
    "encode_df = pd.DataFrame(enc.fit_transform(mastery_df))\n",
    "\n",
    "# Add the encoded variable names to the dataframe\n",
    "encode_df.columns = enc.get_feature_names(mastery_df)\n",
    "encode_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c59001",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge the new encode_df with the diamondPlayerIDs dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8967edd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define x and y\n",
    "y = \n",
    "X = \n",
    "\n",
    "# Split training/test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)\n",
    "\n",
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285d67d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random forest classifier.\n",
    "rf_model = RandomForestClassifier(n_estimators=128, random_state=78)\n",
    "\n",
    "# Fitting the model\n",
    "rf_model = rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = rf_model.predict(X_test_scaled)\n",
    "print(f\" Random forest predictive accuracy: {accuracy_score(y_test,y_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9be422",
   "metadata": {},
   "source": [
    "# vs Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf83f4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model - deep neural net\n",
    "number_input_features = len(X_train_scaled[0])\n",
    "hidden_nodes_layer1 = 24\n",
    "hidden_nodes_layer2 = 12\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(\n",
    "    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\")\n",
    ")\n",
    "\n",
    "# Second hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "\n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the Sequential model together and customize metrics\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "fit_model = nn.fit(X_train_scaled, y_train, epochs=50)\n",
    "\n",
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
